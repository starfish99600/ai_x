{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec95f0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:85% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:85% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00b3b2",
   "metadata": {},
   "source": [
    "<b><font color=\"red\" size=\"6\">ch14. 웹데이터 수집</font></b>\n",
    "# 1절. BeautifulSoup과 parser\n",
    "```pip install bs4``` 아나콘다를 설치하면 7500개여개의 패키지 설치\n",
    "- 공식 사이트 : https://www.crummy.com/software/BeautifulSoup/\n",
    "- Docs : https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ee507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # HTTP 요청 처리 lib\n",
    "from requests_file import FileAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca5eb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = requests.Session() # HTTP 요청관리를 위한 세션 객체\n",
    "s.mount(\"file://\", FileAdapter())\n",
    "response = s.get('file:///ai_x/lecNote/01_python/data/ch14_sample.html')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb7dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code \n",
    "# 200 : 정상\n",
    "# 404 : 없는 페이지\n",
    "# 406 : get, post 오류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cde9c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"[Errno 2] No such file or directory: '\\\\\\\\ai_x\\\\\\\\lecNote\\\\\\\\01_python\\\\\\\\data\\\\\\\\ch14_sample.html'\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content # 바이너리 형식의 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36ddc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Errno 2] No such file or directory: '\\\\\\\\ai_x\\\\\\\\lecNote\\\\\\\\01_python\\\\\\\\data\\\\\\\\ch14_sample.html'\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5468da15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Errno 2] No such file or directory: '\\\\\\\\ai_x\\\\\\\\lecNote\\\\\\\\01_python\\\\\\\\data\\\\\\\\ch14_sample.html'\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73df34a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# html 파싱\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.content,  # response.text\n",
    "                    \"html.parser\")\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7675fa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el : None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m el \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect_one(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# 처음 나오는 h1태그 하나만\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mel :\u001b[39m\u001b[38;5;124m'\u001b[39m, el)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mel.text :\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mel.string :\u001b[39m\u001b[38;5;124m'\u001b[39m, el\u001b[38;5;241m.\u001b[39mstring)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mel의 속성들 :\u001b[39m\u001b[38;5;124m'\u001b[39m, el\u001b[38;5;241m.\u001b[39mattrs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# soup.select_one('선택자') : 해당 선택자 처음 하나만\n",
    "el = soup.select_one('h1') # 처음 나오는 h1태그 하나만\n",
    "print('el :', el)\n",
    "print('el.text :', el.text)\n",
    "print('el.string :', el.string)\n",
    "print('el의 속성들 :', el.attrs)\n",
    "print('el의 title속성 :', el.attrs['title'])\n",
    "print('el의 title속성 :', el.attrs.get('title') )\n",
    "print('el의 이름 :', el.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb67b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.select('선택자') : 해당 선택자 모든 엘리먼트를 리스트로\n",
    "el = soup.select('h1')\n",
    "print('리스트 el :', el)\n",
    "print('el의 text들 :', [e.text for e in el])\n",
    "# for e in el:\n",
    "#     print(e.text, end=',')\n",
    "print('el의 속성들 :', [e.attrs for e in el])\n",
    "print('el의 class속성들 :', [e.attrs.get('class') for e in el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_one(선택자)와 find(태그, 속성)\n",
    "print('select_one :', soup.select_one('h1.css'))\n",
    "print('find :', soup.find('h1', {'class':'css'}))\n",
    "print('find :', soup.find('h1', class_='css'))\n",
    "print()\n",
    "print('select_one :', soup.select_one('h1#text'))\n",
    "print('find :', soup.find('h1', {'id':'text'}));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select(선택자)와 find_all(태그, 속성)\n",
    "print('모든 h1.css, span 태그(select) :',\n",
    "     soup.select('h1.css, span'))\n",
    "print('모든 h1.css, span태그(find_all) :',\n",
    "     soup.find_all(['h1','span'], class_='css'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef302625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 없는 엘리먼트 찾기\n",
    "print('find_all(빈list) :', soup.find_all('a', class_='css'))\n",
    "print('find(None) :', soup.find('a', class_='css'))\n",
    "print('select(빈list) :', soup.select('a.css'))\n",
    "print('select_one(None) :', soup.select_one('a.css'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473adf83",
   "metadata": {},
   "source": [
    "# 2절. 정적 웹 데이터 수집(정적 웹크롤링)\n",
    "- json, html, xml\n",
    "## 2.1 JSON파일\n",
    "- request모듈(get)\n",
    "- urllib.request모듈(urlopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 허용 범위는 사이트마다 ~/robots.txt에서 확인할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd49b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "import requests\n",
    "response = requests.get('http://api.github.com')\n",
    "response, response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "from urllib.request import urlopen\n",
    "response = urlopen('http://api.github.com')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca29b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '{\"속성1\": \"값1\",\"속성2\":\"값2\"}'\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9618ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자(딕셔너리 타입)를 딕셔너리\n",
    "# '{\"속성1\": \"값1\",\"속성2\":\"값2\"}' => {\"속성1\": \"값1\",\"속성2\":\"값2\"}\n",
    "import json\n",
    "json.loads(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef993fb0",
   "metadata": {},
   "source": [
    "## 2.2 html 파일\n",
    "### 1) 환율정보 가져오기 (네이버->증권->시장지표)\n",
    "- https://finance.naver.com/marketindex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307736d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://finance.naver.com/marketindex'\n",
    "response = requests.get(url)\n",
    "# response, response.status_code\n",
    "# response.content \n",
    "# response.content.decode('cp949') == response.text\n",
    "soup = BeautifulSoup(response.text, # response.content\n",
    "                    'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "from urllib.request import urlopen\n",
    "response = urlopen(url)\n",
    "# response.status 상태코드\n",
    "# response.read()\n",
    "# response.read().decode('cp949')\n",
    "soup = BeautifulSoup(response,  # response.read()\n",
    "                     'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.select('h3.h_lst > span.blind')\n",
    "for t in title:\n",
    "    print(t.text, end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = soup.select('div.head_info > span.value')\n",
    "[p.text for p in price]\n",
    "[round(float(p.text.replace(',',''))) for p in price]\n",
    "[round(float(''.join(p.text.split(',')))) for p in price]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = '1,374.70'\n",
    "round(float(''.join(out.split(','))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(float(out.replace(',','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = soup.select('div.head_info > span > span.blind')\n",
    "unit = [u.text for u in unit]\n",
    "unit.insert(7, '') # 7번째 index에 '' 추가\n",
    "unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = soup.select('div.head_info > span.blind')\n",
    "[t.text for t in title]\n",
    "[p.text for p in price]\n",
    "unit\n",
    "[s.string for s in status]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581803ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(title), len(price), len(unit), len(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(title)):\n",
    "    print(\"{}. {} : {}{} - {}\".format(idx+1,\n",
    "                                     title[idx].text,\n",
    "                                     price[idx].text,\n",
    "                                     unit[idx],\n",
    "                                     status[idx].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6451b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, p, u, s  in zip(title, price, unit, status):\n",
    "    print(\"{} : {}{} - {}\".format(t.text,\n",
    "                                 p.text,\n",
    "                                 u,\n",
    "                                 s.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1131cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (t, p, u, s)  in enumerate(zip(title, price, unit, status)):\n",
    "    print(\"{}. {} : {}{} - {}\".format(idx+1,\n",
    "                                t.text,\n",
    "                                p.text,\n",
    "                                u,\n",
    "                                s.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1d7eb",
   "metadata": {},
   "source": [
    "### 2) 이번주 로또번호 출력\n",
    "- https://dhlottery.co.kr/gameResult.do?method=byWin(google에 \"로또번호 당첨번호\" 검색)\n",
    "```\n",
    "    1173회(2025년 05월 24일 추첨)\n",
    "    당첨번호 [1 5 18 20 30 35]\n",
    "    보너스 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1의 soup객체 생성\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://dhlottery.co.kr/gameResult.do?method=byWin'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, # response.text\n",
    "                    \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2의 sou객체 생성\n",
    "from urllib.request import urlopen\n",
    "response = urlopen(url)\n",
    "soup = BeautifulSoup(response, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = soup.select_one('div.win_result > h4 > strong').text # 1173회\n",
    "date = soup.select_one('div.win_result >p.desc').text  #(2025년 05월 24일 추첨)\n",
    "print(times, date)\n",
    "title1 = soup.select_one('div.num.win > strong').text\n",
    "lotto_number = soup.select('div.num.win > p > span')\n",
    "print(title1, [int(lotto.text) for lotto in lotto_number])\n",
    "title2 = soup.select_one('div.num.bonus > strong').string\n",
    "bonus_number = soup.select_one('div.num.bonus > p > span').text\n",
    "print(title2, bonus_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d98682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 select 함수를 find함수로\n",
    "#times = soup.select_one('div.win_result > h4 > strong').text # 1173회\n",
    "win_result = soup.find('div', class_=\"win_result\")\n",
    "times = win_result.find('strong').text\n",
    "# date = soup.select_one('div.win_result >p.desc').text  #(2025년 05월 24일 추첨)\n",
    "date = soup.find('p', class_='desc').text\n",
    "print(times, date)\n",
    "# title1 = soup.select_one('div.num.win > strong').text\n",
    "num_win = soup.find('div', class_=['win'])\n",
    "title1 = num_win.find('strong').text\n",
    "# lotto_number = soup.select('div.num.win > p > span')\n",
    "lotto_number = num_win.find_all('span') # 배열\n",
    "print(title1, [int(lotto.text) for lotto in lotto_number])\n",
    "# title2 = soup.select_one('div.num.bonus > strong').string\n",
    "num_bonus = soup.find('div', class_=['bonus'])\n",
    "title2 = num_bonus.find('strong').text\n",
    "# bonus_number = soup.select_one('div.num.bonus > p > span').text\n",
    "bonus_number = num_bonus.find('span').text\n",
    "print(title2, bonus_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad7e0c",
   "metadata": {},
   "source": [
    "### 3) 다음 검색 리스트\n",
    "```\n",
    "no  title   link\n",
    "0 [비트코인 2025] 백악관 크립토 차르~ https://v.daum.net/v/20250528103907230\n",
    "1 [비즈 나우] 비트코인 2025 컨퍼런~   https://v.daum.net/v/20250528075215864\n",
    "....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "word = '비트코인'\n",
    "url = 'https://search.daum.net/search?w=news&nil_search=btn&DA=NTB&enc=utf8&cluster=y&cluster_page=1&q='+word\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, # response.content\n",
    "                    \"html.parser\")\n",
    "items_find_list = [] # 검색한 결과를 담을 dict list\n",
    "items_el = soup.select('div.item-title > strong.tit-g.clamp-g > a')\n",
    "# len(items_el)\n",
    "for idx, item in enumerate(items_el):\n",
    "    items_find_list.append({'no': idx,\n",
    "                           'title': item.text,\n",
    "                           'link': item.attrs['href']})\n",
    "import pandas as pd\n",
    "pd.DataFrame(items_find_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "word = '비트코인'\n",
    "url = 'https://search.daum.net/search?w=news&nil_search=btn&DA=NTB&enc=utf8&cluster=y&cluster_page=1&q='+word\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, # response.content\n",
    "                    \"html.parser\")\n",
    "items_find_list = [] # 검색한 결과를 담을 2차원 리스트\n",
    "items_el = soup.select('div.item-title > strong.tit-g.clamp-g > a')\n",
    "for idx, item in enumerate(items_el):\n",
    "    items_find_list.append([idx, item.text, item.attrs.get('href')])\n",
    "# items_find_list\n",
    "import pandas as pd\n",
    "pd.DataFrame(items_find_list, columns=['no', 'title', 'link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b706949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 뉴스 검색(키워드, 원하는 페이지수)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "keyword = '비트코인'\n",
    "page = 2\n",
    "# url = f'https://search.daum.net/search?w=news&nil_search=btn&DA=PGD&enc=utf8&cluster=y&q={keyword}&p={page}'\n",
    "# response = requests.get(url)\n",
    "url = f'https://search.daum.net/search?w=news&nil_search=btn&DA=PGD&enc=utf8&cluster=y'\n",
    "params = {'q':keyword, 'p':page}\n",
    "response = requests.get(url, params=params)\n",
    "soup = BeautifulSoup(response.text,  # response.content\n",
    "                    'html.parser')\n",
    "items_find_list = [] # 검색한 결과를 담을 dict list\n",
    "items_el = soup.select('div.item-title > strong.tit-g.clamp-g > a')\n",
    "# len(items_el)\n",
    "for idx, item in enumerate(items_el):\n",
    "    items_find_list.append({'no': (page-1)*10 + idx,\n",
    "                           'title': item.text,\n",
    "                           'link': item.attrs['href']})\n",
    "    print({'no': (page-1)*10 + idx,\n",
    "           'title': item.text,\n",
    "           'link': item.attrs['href']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8990df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "def collect_list(keyword, page=1):\n",
    "    'keyword로 다음 검색한 결과(해당 page)를 return'\n",
    "    url = f'https://search.daum.net/search?w=news&nil_search=btn&DA=PGD&enc=utf8&cluster=y'\n",
    "    params = {'q':keyword, 'p':page}\n",
    "    response = requests.get(url, params=params)\n",
    "    soup = BeautifulSoup(response.text,  # response.content\n",
    "                    'html.parser')\n",
    "    items_find_list = [] # 검색한 결과를 담을 dict list\n",
    "    items_el = soup.select('div.item-title > strong.tit-g.clamp-g > a')\n",
    "    for idx, item in enumerate(items_el):\n",
    "        items_find_list.append({'no': (page-1)*10 + idx,\n",
    "                           'title': item.text,\n",
    "                           'link': item.attrs['href']})\n",
    "    return items_find_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_list('청바지',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 keyword로 다음검색\n",
    "result = []\n",
    "keyword = \"청바지\"\n",
    "pages = 4\n",
    "for page in range(1, pages+1):\n",
    "    result.extend(collect_list(keyword, page))\n",
    "    time.sleep(3)\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e93e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['청바지','면바지']\n",
    "pages = 2\n",
    "result0 = [] # 청바지 검색결과 50개\n",
    "result1 = [] # 면바지 검색결과 50개\n",
    "for i, keyword in enumerate(keywords):\n",
    "    for page in range(1, pages+1):\n",
    "        print(f'~ ~ ~ {i}번째 {keyword} {page} 검색 중입니다 ~ ~ ~')\n",
    "        if i==0:\n",
    "            result0.extend(collect_list(keyword, page))\n",
    "        else:\n",
    "            result1.extend(collect_list(keyword, page))\n",
    "        time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "result0_df = pd.DataFrame(result0)\n",
    "result0_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f5e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1_df = pd.DataFrame(result1)\n",
    "result1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result0_df.to_csv('data/ch14_'+keywords[0]+'.csv', index=False, encoding='cp949')\n",
    "result1_df.to_csv('data/ch14_'+keywords[1]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a1f27",
   "metadata": {},
   "source": [
    "### 4) User-agent를 추가하여 크롤링\n",
    "- urlopen()함수를 사용하면 크롤링이 안 되는 사이트\n",
    "- User-agent를 추가하여 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7946640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "from urllib.request import urlopen, Request\n",
    "import urllib.parse\n",
    "word = '비트코인'\n",
    "word = urllib.parse.quote(word)\n",
    "# print(word)\n",
    "url = 'https://search.daum.net/search?w=news&nil_search=btn&DA=NTB&enc=utf8&cluster=y&cluster_page=1&q='+word\n",
    "# User-Agent를 추가하여 브라우저처럼 보이게 포장\n",
    "headers = {'user-agent':\n",
    "         'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36'}\n",
    "request = Request(url, headers=headers)\n",
    "response = urlopen(request)\n",
    "soup = BeautifulSoup(response, # response.content\n",
    "                    \"html.parser\")\n",
    "def collect_list(keyword, page=1):\n",
    "    'keyword로 다음 검색한 결과(해당 page)를 return'\n",
    "    url = f'https://search.daum.net/search?w=news&nil_search=btn&DA=PGD&enc=utf8&cluster=y'\n",
    "    params = {'q':keyword, 'p':page}\n",
    "    response = requests.get(url, params=params)\n",
    "    soup = BeautifulSoup(response.text,  # response.content\n",
    "                    'html.parser')\n",
    "    items_find_list = [] # 검색한 결과를 담을 dict list\n",
    "    items_el = soup.select('div.item-title > strong.tit-g.clamp-g > a')\n",
    "    for idx, item in enumerate(items_el):\n",
    "        items_find_list.append({'no': (page-1)*10 + idx,\n",
    "                           'title': item.text,\n",
    "                           'link': item.attrs['href']})\n",
    "    return items_find_list\n",
    "collect_list(\"청바지\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e842bfc5",
   "metadata": {},
   "source": [
    "- 꼭 User-Agent를 사용하여야 하는 경우 : https://www.melon.com/chart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://www.melon.com/chart/'\n",
    "melonpage = requests.get(url)\n",
    "melonpage.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a40fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "from urllib.request import urlopen, Request\n",
    "# melonpage = urlopen(url) 에러남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent 추가\n",
    "headers = {'user-agent':\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36'\n",
    "}\n",
    "# 방법2\n",
    "request = Request(url, headers=headers)\n",
    "melonpage = urlopen(request)\n",
    "# 방법1\n",
    "melonpage = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(melonpage.text, # melonpage.content\n",
    "                    \"html.parser\")\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50646be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멜론 순위, 노래제목, 가수, 좋아요갯수\n",
    "# 1위 | 너에게 닿기를 | 10cm (100위까지 크롤링)\n",
    "title_els = soup.select('div.ellipsis.rank01 > span')\n",
    "singer_els = soup.select('div.ellipsis.rank02 > span.checkEllipsis')\n",
    "len([title.text.strip() for title in title_els]), \\\n",
    "len([singer.text for singer in singer_els])\n",
    "for idx, (title, singer) in enumerate(zip(title_els, singer_els)):\n",
    "    print('{}위 | {} - {}'.format(idx+1,\n",
    "                                 title.text.strip(),\n",
    "                                 singer.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c375a4",
   "metadata": {},
   "source": [
    "### 5)네이버 지식IN으로 검색(open API 사용 X)\n",
    "- 특정 keyword를 특정페이지 수(3)만큼 검색한 결과 데이터프레임으로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0628c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "keyword = '챗지피티'\n",
    "url = 'https://kin.naver.com/search/list.naver?query={}'.format(keyword)\n",
    "# print(url)\n",
    "response = get(url)\n",
    "# response.status_code\n",
    "soup = BeautifulSoup(response.text, # response.content\n",
    "                    'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote\n",
    "# 쳇지피티 -> %EC%B3%87%EC%A7%80%ED%94%BC%ED%8B%B0\n",
    "keyword = quote(keyword) # url 인코딩된 문자열로 전환\n",
    "url = 'https://kin.naver.com/search/list.naver?query={}'.format(keyword)\n",
    "# print(url)\n",
    "response = urlopen(url)\n",
    "soup = BeautifulSoup(response, # response.read()\n",
    "                     'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa393e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이징 포함(keyword를 pages수 만큼 검색한 결과를 데이터프레임에 )\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "keyword = '쳇지피티'\n",
    "pages = 3\n",
    "items_list = [] # 크롤링 한 데이터를 담을 list(2차원 리스트)\n",
    "for page in range(1, pages+1):\n",
    "    url = f'https://kin.naver.com/search/list.naver?query={keyword}&page={page}'\n",
    "    # print(url)\n",
    "    # 해당 url의 soup객체 -> 글제목, link를 가져와서 items_list에 append\n",
    "    response = requests.get(url)\n",
    "    # print(response.status_code) 정상 접근 가능한지 여부\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    items = soup.select('dt > a')\n",
    "    # print([item.attrs['href'] for item in items])\n",
    "    for item in items:\n",
    "        items_list.append([item.text, item.attrs.get('href')])\n",
    "#         items_list.append({\n",
    "#                 'title':item.text,\n",
    "#                 'link':item.attrs.get('href')})\n",
    "# print(len(items_list), '개 데이터')\n",
    "# print(items_list[:2])\n",
    "df = pd.DataFrame(items_list, columns=['title', 'link'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이징 포함(keyword를 pages수 만큼 검색한 결과를 데이터프레임에 )\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "keyword = '쳇지피티'\n",
    "pages = 3\n",
    "items_list = [] # 크롤링 한 데이터를 담을 list(2차원 리스트)\n",
    "for page in range(1, pages+1):\n",
    "    url = f'https://kin.naver.com/search/list.naver?query={keyword}&page={page}'\n",
    "    # print(url)\n",
    "    # 해당 url의 soup객체 -> 글제목, link를 가져와서 items_list에 append\n",
    "    response = requests.get(url)\n",
    "    # print(response.status_code) 정상 접근 가능한지 여부\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    dts = soup.find_all('dt')\n",
    "    # print([item.attrs['href'] for item in items])\n",
    "    for dt in dts:\n",
    "        item = dt.find('a')\n",
    "        items_list.append([item.text, item.attrs.get('href')])\n",
    "df = pd.DataFrame(items_list, columns=['title', 'link'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9f473",
   "metadata": {},
   "source": [
    "### 6)네이버 지식IN으로 검색(open API 사용 O)\n",
    "- 특정 keyword를 특정페이지 수(30)만큼 검색한 결과 데이터프레임으로 출력\n",
    "    - .env에 발급받은 키 저장 -> load (pip install python-dotenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ecf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수를 가져오기\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() # dotenv_path='.env' 기본값\n",
    "# print(os.getenv('Client_ID'))\n",
    "# print(os.getenv('Client_Secret'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 검색 API 예제 - 지식인 검색\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "client_id = os.getenv('Client_ID')\n",
    "client_secret = os.getenv('Client_Secret')\n",
    "encText = urllib.parse.quote(\"쳇지피티\")\n",
    "url = \"https://openapi.naver.com/v1/search/kin.json?query=\" + encText # JSON 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/kin.xml?query=\" + encText # XML 결과\n",
    "\n",
    "headers = {'X-NAVER-Client-ID':client_id,\n",
    "          'X-NAVER-Client-Secret':client_secret}\n",
    "request = urllib.request.Request(url, headers=headers)\n",
    "# request = urllib.request.Request(url)\n",
    "# request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "# request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "    print(type(response_body.decode('utf-8')))\n",
    "    import json\n",
    "    data = json.loads(response_body.decode('utf-8')) # json형태의 str을  딕셔너리\n",
    "    print(type(data))\n",
    "    print(data['items'])\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword를 display수만큼 지식in검색 결과를 데이터프레임에 (방법1 크롤링)\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "client_id = os.getenv('Client_ID')\n",
    "client_secret = os.getenv('Client_Secret')\n",
    "keyword = '쳇지피티'\n",
    "cnt = 30\n",
    "url = f'https://openapi.naver.com/v1/search/kin.json?query={keyword}&display={cnt}'\n",
    "headers = {'X-NAVER-Client-ID':client_id,\n",
    "          'X-NAVER-Client-Secret':client_secret}\n",
    "response = requests.get(url, headers=headers)\n",
    "# response.status_code\n",
    "# items = json.loads(response.text)['items'] : json.loads() -방법1과 방법2\n",
    "items = response.json()['items'] # 방법1에서만 가능한 함수\n",
    "items_list = []\n",
    "for item in items:\n",
    "    items_list.append([item.get('title').replace('<b>','').replace('</b>',''), # item['title']\n",
    "                       item.get('link'),\n",
    "                       item.get('description')\n",
    "                      ])\n",
    "df = pd.DataFrame(items_list, columns=['title', 'link', 'description'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법 2\n",
    "def get_naver_kin(keyword, cnt):\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import json \n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()\n",
    "    client_id = os.getenv('Client_ID')\n",
    "    client_secret = os.getenv('Client_Secret')\n",
    "    keyword = '쳇지피티'\n",
    "    cnt = 30\n",
    "    url = f'https://openapi.naver.com/v1/search/kin.json?query={keyword}&display={cnt}'\n",
    "    headers = {'X-NAVER-Client-ID':client_id,\n",
    "              'X-NAVER-Client-Secret':client_secret}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # response.status_code\n",
    "    # items = json.loads(response.text)['items'] : json.loads() -방법1과 방법2\n",
    "    items = response.json()['items'] # 방법1에서만 가능한 함수\n",
    "    items_list = []\n",
    "    for item in items:\n",
    "        items_list.append([item.get('title').replace('<b>','').replace('</b>',''), # item['title']\n",
    "                           item.get('link'),\n",
    "                           item.get('description').replace('<b>','').replace('<b>','')\n",
    "                          ])\n",
    "    return pd.DataFrame(items_list, columns=['title', 'link', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4559dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_naver_kin(\"청바지\",20).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c6e86",
   "metadata": {},
   "source": [
    "### quiz) 네이버 open API를 이용해서 청바지 이미지 100건 데이터를 csv파일로 저장\n",
    "        데이터 : 제목, 이미지링크, 썸네일링크, 높이, 폭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_naver_image(keyword, cnt):\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "\n",
    "    load_dotenv()\n",
    "    client_id = os.getenv('Client_ID')\n",
    "    client_secret = os.getenv('Client_Secret')\n",
    "    \n",
    "    keyword = '청바지 이미지'\n",
    "    cnt = 100\n",
    "\n",
    "    url = f'https://openapi.naver.com/v1/search/image?query={keyword}&display={cnt}'\n",
    "    headers = {'X-Naver-Client-Id': client_id,\n",
    "              'X-Naver-Client-Secret': client_secret}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    items = response.json()['items']\n",
    "    for item in items:\n",
    "        items_list.append([\n",
    "            item.get('title').replace('<b>', '').replace('</b>', ''),\n",
    "            item.get('link'),\n",
    "            item.get('thumbnail'),\n",
    "            item.get('sizeheight'),\n",
    "            item.get('sizewidth')\n",
    "        ])\n",
    "\n",
    "    return pd.DataFrame(items_list, columns=['title', 'link', 'thumbnail', 'height', 'width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f43826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_naver_image('청바지', 100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889e9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4ee8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ecc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() \n",
    "client_id = os.getenv('Client_ID')\n",
    "client_secret = os.getenv('Client_Secret')\n",
    "word = \"청바지\"\n",
    "url = f'https://openapi.naver.com/v1/search/image?query={word}&display=100'\n",
    "headers = {'X-Naver-Client-Id': client_id,\n",
    "           'X-Naver-Client-Secret':client_secret}\n",
    "response = requests.get(url, headers=headers)\n",
    "# print(response.text)\n",
    "items = response.json()['items'] # 방법1에서만 가능한 함수\n",
    "items_list = []\n",
    "for idx, item in enumerate(items):\n",
    "    title = item.get('title')\n",
    "    link = item.get('link')\n",
    "    thumbnail = item.get('thumbnail')\n",
    "    sizeheight = item.get('sizeheight')\n",
    "    sizewidth = item.get('sizewidth')\n",
    "    items_list.append({\n",
    "        'no': idx+1,\n",
    "        'title': title,\n",
    "        'link' : link,\n",
    "        'thumbnail' : thumbnail,\n",
    "        'sizeheight':sizeheight,\n",
    "        'sizewidth' :sizewidth\n",
    "    })\n",
    "    # link와 thumbnail을 저장\n",
    "df = pd.DataFrame(items_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fe709",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[3, 'link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['link'].str.find('?')!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428447a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df['link'].str.endswith('jpg')].loc[5, 'link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(attr, idx, query, url):\n",
    "    import requests\n",
    "    '{attr}_{idx}_{query}.{확장자} 이미지 저장'\n",
    "    file_extension = url.split('.')[-1]\n",
    "    quote_index = file_extension.find('?')\n",
    "    \n",
    "    if quote_index != -1:\n",
    "        file_extension = file_extension[:quote_index]\n",
    "#     print(file_extension)\n",
    "    img = requests.get(url).content\n",
    "    open(f'ch14_image/{attr}_{idx}_{query}.{file_extension}', 'wb').write(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b899827",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img('메인', 1, '청바지', df.loc[5, 'link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word image 100개 검색한 데이터는 csv파일로, 이미지는 ch14_image폴더에 저장되는 함수\n",
    "def get_naver_save_image(word):\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv() \n",
    "    client_id = os.getenv('Client_ID')\n",
    "    client_secret = os.getenv('Client_Secret')\n",
    "    url = f'https://openapi.naver.com/v1/search/image?query={word}&display=100'\n",
    "    headers = {'X-Naver-Client-Id': client_id,\n",
    "               'X-Naver-Client-Secret':client_secret}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    items = response.json()['items'] # 방법1에서만 가능한 함수\n",
    "    items_list = []\n",
    "    for idx, item in enumerate(items):\n",
    "        title = item.get('title')\n",
    "        link = item.get('link')\n",
    "        thumbnail = item.get('thumbnail')\n",
    "        sizeheight = item.get('sizeheight')\n",
    "        sizewidth = item.get('sizewidth')\n",
    "        items_list.append({\n",
    "            'no': idx+1,\n",
    "            'title': title,\n",
    "            'link' : link,\n",
    "            'thumbnail' : thumbnail,\n",
    "            'sizeheight':sizeheight,\n",
    "            'sizewidth' :sizewidth\n",
    "        })\n",
    "        # link와 thumbnail을 저장\n",
    "        save_img('메인', idx+1, word, link)\n",
    "        save_img('썸네일', idx+1, word, thumbnail)\n",
    "        # 20%, 40%, 60%, 80% 지점 message 출력\n",
    "        if idx/20 == round(idx/20):\n",
    "            print(f'= = = {idx}% 진행 중입니다. = = =')\n",
    "    df = pd.DataFrame(items_list)\n",
    "    df.to_csv(f'ch14_image/{word}.csv', encoding='cp949', index=False)\n",
    "    print('~ ~ ~ 이미지 및 csv 파일 저장 완료 ~ ~ ~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_naver_save_image('청바지')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc14c63",
   "metadata": {},
   "source": [
    "## 2.3 xml 파일\n",
    "### 1) 기상예측 RSS 활용\n",
    "- http://www.kma.go.kr/repositary/xml/fct/mon/img/fct_mon1rss_108_20250529.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab0ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.kma.go.kr/repositary/xml/fct/mon/img/fct_mon1rss_108_20250529.xml'\n",
    "response = urlopen(url)\n",
    "#response.status #response.getcode()\n",
    "soup = BeautifulSoup(response, \"xml\") # pip install lxml\n",
    "locals = soup.find_all('local_ta')\n",
    "# locals[1]\n",
    "items_list = []\n",
    "for local in locals:\n",
    "    local_ta_name = local.select_one('local_ta_name')\n",
    "    week1_normalYear = local.select_one('week1_local_ta_normalYear') # 평년기온\n",
    "    week1_similarRange = local.select_one('week1_local_ta_similarRange') # 기온범위\n",
    "    week1_min = local.select_one('week1_local_ta_minVal')\n",
    "    week1_max = local.select_one('week1_local_ta_maxVal')\n",
    "    week2_normalYear = local.select_one('week2_local_ta_normalYear') # 평년기온\n",
    "    week2_similarRange = local.select_one('week2_local_ta_similarRange') # 기온범위\n",
    "    week2_min = local.select_one('week2_local_ta_minVal')\n",
    "    week2_min = local.select_one('week2_local_ta_maxVal')\n",
    "    items_list.append({\n",
    "        '지역' : local_ta_name,\n",
    "        '1주평년기온' : week1_normalYear,\n",
    "        '1주 범위' : week1_similarRange,\n",
    "        '1주 최저' : week1_min,\n",
    "        '1주 최고' : week1_max,\n",
    "        '2주평년기온' : week2_normalYear,\n",
    "        '2주 범위' : week2_similarRange,\n",
    "        '2주 최저' : week2_min,\n",
    "        '2주 최고' : week2_min\n",
    "    })\n",
    "df = pd.DataFrame(items_list)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524523b",
   "metadata": {},
   "source": [
    "# 3. 연습문제\n",
    "- yes24의 베스트셀러 정보를 ch14_yes24.csv(여유가 된다면 ch14_yes24.txt)로 출력\n",
    "- 작업순서 : 순위(1~48위까지), 책이름, 저장, 출판사, 가격 = > 데이터프레임 => csv => txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d051e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "pages = 2\n",
    "bestseller_list = []\n",
    "with open('data/ch14_yes24.txt', 'w', encoding='utf-8') as f:\n",
    "    pass\n",
    "for page in range(1, pages+1):\n",
    "    url = f'https://www.yes24.com/product/category/bestseller?pageNumber={page}'\n",
    "    response = requests.get(url)\n",
    "    # print(url)\n",
    "    # print(response.status_code)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    ranks_els = soup.select('div.img_upper > em.ico.rank')\n",
    "    ranks = [int(ranks_el.text) for ranks_el in ranks_els]\n",
    "    titles_els=soup.select(\"div.item_info > div.info_row > a.gd_name\")\n",
    "    titles = [titles_el.text for titles_el in titles_els]\n",
    "    writers_els = soup.select(\"span.authPub.info_auth\")\n",
    "    # writers_els = soup.find_all('span', class_='info_auth')\n",
    "    writers = [writers_el.text.strip() for writers_el in writers_els]\n",
    "    publishers_els = soup.select('div.info_row.info_pubGrp > span.authPub.info_pub')\n",
    "    publishers = [publishers_el.text for publishers_el in publishers_els]\n",
    "    prices_els = soup.select(\"div.info_row > strong.txt_num\")\n",
    "    prices = [prices_el.text for prices_el in prices_els]\n",
    "    # print(len(ranks), len(titles), len(writers), len(publishers), len(prices))\n",
    "    with open('data/ch14_yes24.txt', 'a', encoding='utf-8') as f:\n",
    "        for rank, title, writer, publisher, price in zip(ranks, titles, writers, publishers, prices):\n",
    "            # print(\"{},{},{},{},{}\".format(rank, title, writer, publisher, price))\n",
    "            f.write(f'{rank}, {title}, {writer}, {publisher}, {price}\\n')\n",
    "            bestseller_list.append([rank, title, writer, publisher, price])\n",
    "df = pd.DataFrame(bestseller_list, \n",
    "                  columns=['rank', 'title', 'writer', 'publisher', 'price'])\n",
    "df.to_csv('data/ch14_yes24.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c44b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "pages = 2\n",
    "bestseller_list = []\n",
    "with open('data/ch14_yes24.txt', 'w', encoding='utf-8') as f:\n",
    "    pass\n",
    "for page in range(1, pages+1):\n",
    "    url = f'https://www.yes24.com/product/category/bestseller?pageNumber={page}'\n",
    "    # print(url)\n",
    "    response = urlopen(url)\n",
    "    #print(response.status)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    ranks_els = soup.select('div.img_upper > em.ico.rank')\n",
    "    ranks = [int(ranks_el.text) for ranks_el in ranks_els]\n",
    "    titles_els=soup.select(\"div.item_info > div.info_row > a.gd_name\")\n",
    "    titles = [titles_el.text for titles_el in titles_els]\n",
    "    writers_els = soup.select(\"span.authPub.info_auth\")\n",
    "    # writers_els = soup.find_all('span', class_='info_auth')\n",
    "    writers = [writers_el.text.strip() for writers_el in writers_els]\n",
    "    publishers_els = soup.select('div.info_row.info_pubGrp > span.authPub.info_pub')\n",
    "    publishers = [publishers_el.text for publishers_el in publishers_els]\n",
    "    prices_els = soup.select(\"div.info_row > strong.txt_num\")\n",
    "    prices = [prices_el.text for prices_el in prices_els]\n",
    "    # print(len(ranks), len(titles), len(writers), len(publishers), len(prices))\n",
    "    with open('data/ch14_yes24.txt', 'a', encoding='utf-8') as f:\n",
    "        for rank, title, writer, publisher, price in zip(ranks, titles, writers, publishers, prices):\n",
    "            # print(\"{},{},{},{},{}\".format(rank, title, writer, publisher, price))\n",
    "            bestseller_list.append({\n",
    "                'rank':rank, \n",
    "                'title':title,\n",
    "                'writer':writer,\n",
    "                'publisher':publisher,\n",
    "                'price':price})\n",
    "            f.write(f'{rank}, {title}, {writer}, {publisher}, {price}\\n')\n",
    "df = pd.DataFrame(bestseller_list)\n",
    "df.to_csv('data/ch14_yes24.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbdcacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d0467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a68e265a",
   "metadata": {},
   "source": [
    "### 2) 버스위치정보 조회(OpenAPI활용)\n",
    "- data.go.kr\n",
    "    * 서울특별시_노선정보조회 서비스(busRouteId, stationNm-정류소명, station-정류소 id)\n",
    "    * 서울특별시_버스위치정보조회 서비스(busRouteId->버스들위치)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "634d0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 : 버스번호 -> busRouteId\n",
    "# 노선정보조회 서비스 3번 getBusRouteList서비스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4bb883a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() # 디폴트값 : dotenv_path='.env'\n",
    "# print('key = ', os.getenv('key'))\n",
    "# print('skey = ', os.getenv('skey'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fafadb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "busRouteId : 100100034\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve # url 경로파일을 내 pc에 저장\n",
    "from urllib.parse import quote\n",
    "busNum = quote('마포01')\n",
    "busNum = '162'\n",
    "key = os.getenv('key')\n",
    "\n",
    "url1 = f'http://ws.bus.go.kr/api/rest/busRouteInfo/getBusRouteList?ServiceKey={key}&strSrch={busNum}'\n",
    "# print(url)\n",
    "#response = requests.get(url)\n",
    "#soup = BeautifulSoup(response.text, \"xml\")\n",
    "savefilename = 'data/ch14_busInfo.xml'\n",
    "urlretrieve(url1, savefilename) #url경로 파일을 savefilename으로 저장\n",
    "xml = open(savefilename, 'r', encoding='utf-8').read()\n",
    "soup = BeautifulSoup(xml, \"xml\")\n",
    "\n",
    "for item in soup.select('itemList'):\n",
    "    busRouteNm = item.select_one('busRouteNm').text\n",
    "    if busRouteNm == busNum:\n",
    "        busRouteId = item.select_one('busRouteId').text\n",
    "        break\n",
    "print('busRouteId :', busRouteId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46633524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2 : busRouteId로 버스 정류장 목록(정류장이름과 정류장 id)\n",
    "# 노선정보조회 서비스 4번 getStaionsByRouteList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "746be4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ws.bus.go.kr/api/rest/busRouteInfo/getStaionByRoute?ServiceKey=kWiLqR5an3qgHfkycjPmjxAie5mGGzW0LuPktrSNA3kqpsmWfsLSW9TxR9k0khrZgJLecVcO88n5LsStz845eg%3D%3D&busRouteId=100100034\n",
      "162 번 정류장 갯수 : 77\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>정류소명</th>\n",
       "      <th>id</th>\n",
       "      <th>경도</th>\n",
       "      <th>위도</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>정릉산장아파트</td>\n",
       "      <td>107000071</td>\n",
       "      <td>127.003343</td>\n",
       "      <td>37.616712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>정릉4동주민센터.경국사</td>\n",
       "      <td>107000073</td>\n",
       "      <td>127.006345</td>\n",
       "      <td>37.613529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>북한산보국문역2번출구</td>\n",
       "      <td>107000518</td>\n",
       "      <td>127.0079858233</td>\n",
       "      <td>37.612293899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>성북청수도서관.정릉4동성당</td>\n",
       "      <td>107000075</td>\n",
       "      <td>127.0084193769</td>\n",
       "      <td>37.6115696748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>정릉시장입구</td>\n",
       "      <td>107000077</td>\n",
       "      <td>127.0098212542</td>\n",
       "      <td>37.6084653256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             정류소명         id              경도             위도\n",
       "0         정릉산장아파트  107000071      127.003343      37.616712\n",
       "1    정릉4동주민센터.경국사  107000073      127.006345      37.613529\n",
       "2     북한산보국문역2번출구  107000518  127.0079858233   37.612293899\n",
       "3  성북청수도서관.정릉4동성당  107000075  127.0084193769  37.6115696748\n",
       "4          정릉시장입구  107000077  127.0098212542  37.6084653256"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url2 = f'http://ws.bus.go.kr/api/rest/busRouteInfo/getStaionByRoute?ServiceKey={key}&busRouteId={busRouteId}'\n",
    "print(url2)\n",
    "response = requests.get(url2)\n",
    "soup = BeautifulSoup(response.text, \"xml\")\n",
    "itemList = soup.find_all('itemList')\n",
    "print(busNum, '번 정류장 갯수 :', len(itemList))\n",
    "bus_station = []\n",
    "for itemList in itemList:\n",
    "    # 정류장이름, 정류장 id, 경도, 위도\n",
    "    stationNm = itemList.find('stationNm').text\n",
    "    station = itemList.find('station').text\n",
    "    gpsX = itemList.find('gpsX').text\n",
    "    gpsY = itemList.find('gpsY').text\n",
    "    bus_station.append([stationNm, station, gpsX, gpsY])\n",
    "df = pd.DataFrame(bus_station, columns=['정류소명', 'id', '경도', '위도'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b65b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3 : busRouteId로 버스 위치 정보 조회 2번 getBusPosByRtidList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a43612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ws.bus.go.kr/api/rest/buspos/getBusPosByRtid?ServiceKey=kWiLqR5an3qgHfkycjPmjxAie5mGGzW0LuPktrSNA3kqpsmWfsLSW9TxR9k0khrZgJLecVcO88n5LsStz845eg%3D%3D&busRouteId=100100034\n",
      "운행중인 버스 수: 21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>차량번호</th>\n",
       "      <th>혼잡도</th>\n",
       "      <th>경도</th>\n",
       "      <th>위도</th>\n",
       "      <th>최종정류소id</th>\n",
       "      <th>다음정류소id</th>\n",
       "      <th>도착소요시간</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>서울70사6556</td>\n",
       "      <td>여유</td>\n",
       "      <td>127.01146</td>\n",
       "      <td>37.605178</td>\n",
       "      <td>107000079</td>\n",
       "      <td>107000168</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>서울74사3008</td>\n",
       "      <td>여유</td>\n",
       "      <td>127.016139</td>\n",
       "      <td>37.59374</td>\n",
       "      <td>107000174</td>\n",
       "      <td>101000042</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>서울74사1537</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.990508</td>\n",
       "      <td>37.577498</td>\n",
       "      <td>100000002</td>\n",
       "      <td>101000042</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>서울74사1618</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.982914</td>\n",
       "      <td>37.572228</td>\n",
       "      <td>100000106</td>\n",
       "      <td>101000042</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>서울74사3372</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.972538</td>\n",
       "      <td>37.557193</td>\n",
       "      <td>101000008</td>\n",
       "      <td>102000018</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        차량번호 혼잡도          경도         위도    최종정류소id    다음정류소id 도착소요시간\n",
       "0  서울70사6556  여유   127.01146  37.605178  107000079  107000168    103\n",
       "1  서울74사3008  여유  127.016139   37.59374  107000174  101000042   1399\n",
       "2  서울74사1537  여유  126.990508  37.577498  100000002  101000042    833\n",
       "3  서울74사1618  여유  126.982914  37.572228  100000106  101000042    324\n",
       "4  서울74사3372  여유  126.972538  37.557193  101000008  102000018    697"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url3 = f'http://ws.bus.go.kr/api/rest/buspos/getBusPosByRtid?ServiceKey={key}&busRouteId={busRouteId}'\n",
    "print(url3)\n",
    "response = requests.get(url3)\n",
    "soup = BeautifulSoup(response.text, \"xml\")\n",
    "bus_position = []\n",
    "itemList = soup.select('itemList')\n",
    "print('운행중인 버스 수:', len(itemList))\n",
    "for itemList in soup.select('itemList'):\n",
    "    # 차량번호, 혼잡도, 경도, 위도, 최종정류장id(lastStnId), 다음정류장 id, 도착소요시간\n",
    "    plainNo = itemList.select_one('plainNo').text\n",
    "    # print(plainNo)\n",
    "    congetion = itemList.select_one('congetion').text\n",
    "    congetion = '여유' if congetion == '3' \\\n",
    "            else \"보통\" if congetion=='4' \\\n",
    "            else \"혼잡\" if congetion=='5' \\\n",
    "            else '매우혼잡'\n",
    "    # print(congetion)\n",
    "    gpsX = itemList.select_one('gpsX').text\n",
    "    gpsY = itemList.select_one('gpsY').text\n",
    "    lastStnId = itemList.select_one('lastStnId').text\n",
    "    nextStId = itemList.select_one('nextStId').text\n",
    "    nextStTm = itemList.select_one('nextStTm').text\n",
    "    bus_position.append({\n",
    "        '차량번호' : plainNo,\n",
    "        '혼잡도' : congetion,\n",
    "        '경도' : gpsX,\n",
    "        '위도' : gpsY,\n",
    "        '최종정류소id' : lastStnId,\n",
    "        '다음정류소id' : nextStId,\n",
    "        '도착소요시간' : nextStTm\n",
    "    })\n",
    "df_pos = pd.DataFrame(bus_position)\n",
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "881fa2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'정릉우체국앞'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id가 107000079인 정류소명\n",
    "df.loc[df['id'] == '107000079', '정류소명'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f811799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def station_name(row):\n",
    "    row['최종정류소명'] = df.loc[df['id'] == row['최종정류소id'], '정류소명'].iloc[0]\n",
    "    row['다음정류소명'] = df.loc[df['id'] == row['다음정류소id'], '정류소명'].iloc[0]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ef004a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "차량번호       서울70사6556\n",
       "혼잡도               여유\n",
       "경도         127.01146\n",
       "위도         37.605178\n",
       "최종정류소id    107000079\n",
       "다음정류소id    107000168\n",
       "도착소요시간           103\n",
       "최종정류소명        정릉우체국앞\n",
       "다음정류소명      정릉입구.정릉역\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_name(df_pos.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cffe1ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>차량번호</th>\n",
       "      <th>혼잡도</th>\n",
       "      <th>경도</th>\n",
       "      <th>위도</th>\n",
       "      <th>최종정류소id</th>\n",
       "      <th>다음정류소id</th>\n",
       "      <th>도착소요시간</th>\n",
       "      <th>최종정류소명</th>\n",
       "      <th>다음정류소명</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>서울74사3008</td>\n",
       "      <td>여유</td>\n",
       "      <td>127.016139</td>\n",
       "      <td>37.59374</td>\n",
       "      <td>107000174</td>\n",
       "      <td>101000042</td>\n",
       "      <td>1399</td>\n",
       "      <td>성신여대입구역</td>\n",
       "      <td>해운센터.롯데영플라자</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        차량번호 혼잡도          경도        위도    최종정류소id    다음정류소id 도착소요시간   최종정류소명  \\\n",
       "1  서울74사3008  여유  127.016139  37.59374  107000174  101000042   1399  성신여대입구역   \n",
       "\n",
       "        다음정류소명  \n",
       "1  해운센터.롯데영플라자  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos = df_pos.apply(station_name, axis=1)\n",
    "df_pos.samplemple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "165e0074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>차량번호</th>\n",
       "      <th>혼잡도</th>\n",
       "      <th>경도</th>\n",
       "      <th>위도</th>\n",
       "      <th>도착소요시간</th>\n",
       "      <th>최종정류소명</th>\n",
       "      <th>다음정류소명</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>서울70사6556</td>\n",
       "      <td>여유</td>\n",
       "      <td>127.01146</td>\n",
       "      <td>37.605178</td>\n",
       "      <td>103</td>\n",
       "      <td>정릉우체국앞</td>\n",
       "      <td>정릉입구.정릉역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>서울74사3008</td>\n",
       "      <td>여유</td>\n",
       "      <td>127.016139</td>\n",
       "      <td>37.59374</td>\n",
       "      <td>1399</td>\n",
       "      <td>성신여대입구역</td>\n",
       "      <td>해운센터.롯데영플라자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>서울74사1537</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.990508</td>\n",
       "      <td>37.577498</td>\n",
       "      <td>833</td>\n",
       "      <td>창경궁.서울대학교병원</td>\n",
       "      <td>해운센터.롯데영플라자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>서울74사1618</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.982914</td>\n",
       "      <td>37.572228</td>\n",
       "      <td>324</td>\n",
       "      <td>조계사.종로경찰서</td>\n",
       "      <td>해운센터.롯데영플라자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>서울74사3372</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.972538</td>\n",
       "      <td>37.557193</td>\n",
       "      <td>697</td>\n",
       "      <td>숭례문</td>\n",
       "      <td>남영역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>서울70사6557</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.970843</td>\n",
       "      <td>37.541809</td>\n",
       "      <td>837</td>\n",
       "      <td>남영역</td>\n",
       "      <td>시범아파트.대교아파트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>서울74사3366</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.947522</td>\n",
       "      <td>37.52812</td>\n",
       "      <td>195</td>\n",
       "      <td>원효로3가</td>\n",
       "      <td>시범아파트.대교아파트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>서울74사1604</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.926857</td>\n",
       "      <td>37.519012</td>\n",
       "      <td>1001</td>\n",
       "      <td>샛강역1번출구.여의도자이</td>\n",
       "      <td>여의도역6번출구</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>서울70사6549</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.924356</td>\n",
       "      <td>37.521786</td>\n",
       "      <td>897</td>\n",
       "      <td>여의도역5번출구</td>\n",
       "      <td>여의도역6번출구</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>서울74사2296</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.92323</td>\n",
       "      <td>37.522601</td>\n",
       "      <td>215</td>\n",
       "      <td>한국경제인협회</td>\n",
       "      <td>여의도역6번출구</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>서울74사2220</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.966094</td>\n",
       "      <td>37.53782</td>\n",
       "      <td>447</td>\n",
       "      <td>용산e편한세상</td>\n",
       "      <td>남영역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>서울74사8118</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.970183</td>\n",
       "      <td>37.541131</td>\n",
       "      <td>280</td>\n",
       "      <td>용산경찰서</td>\n",
       "      <td>남영역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>서울70사6554</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.972633</td>\n",
       "      <td>37.543045</td>\n",
       "      <td>291</td>\n",
       "      <td>숙대입구역</td>\n",
       "      <td>서울역버스환승센터.강우규의거터</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>서울74사3032</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.980569</td>\n",
       "      <td>37.561475</td>\n",
       "      <td>178</td>\n",
       "      <td>남대문시장앞.이회영활동터</td>\n",
       "      <td>명동.롯데영플라자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>서울70사6553</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.982527</td>\n",
       "      <td>37.565023</td>\n",
       "      <td>1119</td>\n",
       "      <td>명동.롯데영플라자</td>\n",
       "      <td>혜화역2번출구.마로니에공원</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>서울70사6560</td>\n",
       "      <td>여유</td>\n",
       "      <td>126.989616</td>\n",
       "      <td>37.577307</td>\n",
       "      <td>475</td>\n",
       "      <td>창덕궁.우리소리박물관</td>\n",
       "      <td>혜화역2번출구.마로니에공원</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>서울74사3360</td>\n",
       "      <td>여유</td>\n",
       "      <td>127.015601</td>\n",
       "      <td>37.592409</td>\n",
       "      <td>553</td>\n",
       "      <td>삼선교.한성대학교.조소앙활동터</td>\n",
       "      <td>아리랑고개.아리랑시네미디어센터</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>서울74사2216</td>\n",
       "      <td>여유</td>\n",
       "      <td>127.01365</td>\n",
       "      <td>37.600443</td>\n",
       "      <td>116</td>\n",
       "      <td>아리랑고개.아리랑시네미디어센터</td>\n",
       "      <td>정릉입구.정릉역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>서울74사2218</td>\n",
       "      <td>여유</td>\n",
       "      <td>127.013142</td>\n",
       "      <td>37.603783</td>\n",
       "      <td>117</td>\n",
       "      <td>정릉입구.정릉역</td>\n",
       "      <td>정릉입구.정릉역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>서울70사6558</td>\n",
       "      <td>여유</td>\n",
       "      <td>127.010107</td>\n",
       "      <td>37.608024</td>\n",
       "      <td>117</td>\n",
       "      <td>정릉시장입구</td>\n",
       "      <td>정릉입구.정릉역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>서울74사1536</td>\n",
       "      <td>여유</td>\n",
       "      <td>127.005169</td>\n",
       "      <td>37.616112</td>\n",
       "      <td>119</td>\n",
       "      <td>정릉4동주민센터.경국사</td>\n",
       "      <td>정릉입구.정릉역</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         차량번호 혼잡도          경도         위도 도착소요시간            최종정류소명  \\\n",
       "0   서울70사6556  여유   127.01146  37.605178    103            정릉우체국앞   \n",
       "1   서울74사3008  여유  127.016139   37.59374   1399           성신여대입구역   \n",
       "2   서울74사1537  여유  126.990508  37.577498    833       창경궁.서울대학교병원   \n",
       "3   서울74사1618  여유  126.982914  37.572228    324         조계사.종로경찰서   \n",
       "4   서울74사3372  여유  126.972538  37.557193    697               숭례문   \n",
       "5   서울70사6557  여유  126.970843  37.541809    837               남영역   \n",
       "6   서울74사3366  여유  126.947522   37.52812    195             원효로3가   \n",
       "7   서울74사1604  여유  126.926857  37.519012   1001     샛강역1번출구.여의도자이   \n",
       "8   서울70사6549  여유  126.924356  37.521786    897          여의도역5번출구   \n",
       "9   서울74사2296  여유   126.92323  37.522601    215           한국경제인협회   \n",
       "10  서울74사2220  여유  126.966094   37.53782    447           용산e편한세상   \n",
       "11  서울74사8118  여유  126.970183  37.541131    280             용산경찰서   \n",
       "12  서울70사6554  여유  126.972633  37.543045    291             숙대입구역   \n",
       "13  서울74사3032  여유  126.980569  37.561475    178     남대문시장앞.이회영활동터   \n",
       "14  서울70사6553  여유  126.982527  37.565023   1119         명동.롯데영플라자   \n",
       "15  서울70사6560  여유  126.989616  37.577307    475       창덕궁.우리소리박물관   \n",
       "16  서울74사3360  여유  127.015601  37.592409    553  삼선교.한성대학교.조소앙활동터   \n",
       "17  서울74사2216  여유   127.01365  37.600443    116  아리랑고개.아리랑시네미디어센터   \n",
       "18  서울74사2218  여유  127.013142  37.603783    117          정릉입구.정릉역   \n",
       "19  서울70사6558  여유  127.010107  37.608024    117            정릉시장입구   \n",
       "20  서울74사1536  여유  127.005169  37.616112    119      정릉4동주민센터.경국사   \n",
       "\n",
       "              다음정류소명  \n",
       "0           정릉입구.정릉역  \n",
       "1        해운센터.롯데영플라자  \n",
       "2        해운센터.롯데영플라자  \n",
       "3        해운센터.롯데영플라자  \n",
       "4                남영역  \n",
       "5        시범아파트.대교아파트  \n",
       "6        시범아파트.대교아파트  \n",
       "7           여의도역6번출구  \n",
       "8           여의도역6번출구  \n",
       "9           여의도역6번출구  \n",
       "10               남영역  \n",
       "11               남영역  \n",
       "12  서울역버스환승센터.강우규의거터  \n",
       "13         명동.롯데영플라자  \n",
       "14    혜화역2번출구.마로니에공원  \n",
       "15    혜화역2번출구.마로니에공원  \n",
       "16  아리랑고개.아리랑시네미디어센터  \n",
       "17          정릉입구.정릉역  \n",
       "18          정릉입구.정릉역  \n",
       "19          정릉입구.정릉역  \n",
       "20          정릉입구.정릉역  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 컬럼명에 id가 포함된 컬럼 제거\n",
    "drop_true = df_pos.columns.str.contains('id')\n",
    "drop_column_name = df_pos.columns[drop_true]\n",
    "df_pos.drop(drop_column_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ad962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821c007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26712a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885a7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbb348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74428d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f686b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c037a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
