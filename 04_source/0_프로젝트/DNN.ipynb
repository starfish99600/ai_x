{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562f7759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:86% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:130px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:15px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:86% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:130px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:15px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52286542",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LSTM최종데이터_X_Y_라벨링등전.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 데이터 로딩 및 전처리\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLSTM최종데이터_X_Y_라벨링등전.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m대여량\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m일시\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m일시\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LSTM최종데이터_X_Y_라벨링등전.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "import holidays\n",
    "import tensorflow as tf\n",
    "\n",
    "# 데이터 로딩 및 전처리\n",
    "df = pd.read_csv('LSTM최종데이터_X_Y_라벨링등전.csv')\n",
    "df = df.dropna(subset=['대여량'])\n",
    "\n",
    "df['일시'] = pd.to_datetime(df['일시'])\n",
    "df['연'] = df['일시'].dt.year\n",
    "df['월'] = df['일시'].dt.month\n",
    "df['일'] = df['일시'].dt.day\n",
    "df['시'] = df['일시'].dt.hour\n",
    "df['요일'] = df['일시'].dt.dayofweek\n",
    "start_year = df['연'].min()\n",
    "end_year = df['연'].max()\n",
    "kr_holidays = holidays.KR(years=range(start_year, end_year+1))\n",
    "df['공휴일'] = df['일시'].dt.floor('D').isin(kr_holidays).astype(int)\n",
    "\n",
    "df['key'] = df['행정구'] + '_' + df['월'].astype(str) + '_' + df['일'].astype(str) + '_' + df['시'].astype(str)\n",
    "df['일시_1년전'] = df['일시'] - pd.DateOffset(years=1)\n",
    "df['key_1년전'] = df['행정구'] + '_' + df['일시_1년전'].dt.month.astype(str) + '_' + df['일시_1년전'].dt.day.astype(str) + '_' + df['일시_1년전'].dt.hour.astype(str)\n",
    "cols_to_copy = ['key', '대여량', '총생활인구수']\n",
    "df_1y = df[cols_to_copy].copy()\n",
    "df_1y.columns = [c + '_1년전' if c != 'key' else 'key_1년전' for c in cols_to_copy]\n",
    "df = pd.merge(df, df_1y, how='left', on='key_1년전')\n",
    "df = df.dropna(subset=['대여량_1년전', '총생활인구수_1년전'])\n",
    "\n",
    "# 범주형 구간 자동(9개) 생성 (qcut 사용)\n",
    "num_bins = 9\n",
    "df['대여량_class'], bins = pd.qcut(df['대여량'], q=num_bins, labels=False, retbins=True, duplicates='drop')\n",
    "print(\"대여량 구간 경계값:\", bins)\n",
    "\n",
    "# 구간별 데이터 분포 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',family='Hancom Gothic')\n",
    "plt.figure(figsize=(8,4))\n",
    "df['대여량_class'].value_counts(sort=False).plot(kind='bar')\n",
    "plt.title('대여량 클래스별 샘플수 (qcut)')\n",
    "plt.xlabel('대여량 클래스 index')\n",
    "plt.ylabel('샘플 수')\n",
    "plt.show()\n",
    "\n",
    "# 파생 변수 생성 및 인코딩\n",
    "df['인구_증감'] = df['총생활인구수'] - df['총생활인구수_1년전']\n",
    "df['인구_증감률'] = (df['총생활인구수'] - df['총생활인구수_1년전']) / (df['총생활인구수_1년전'] + 1)\n",
    "df['대여량_증감'] = df['대여량'] - df['대여량_1년전']\n",
    "df['대여량_증감률'] = (df['대여량'] - df['대여량_1년전']) / (df['대여량_1년전'] + 1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "label_cols = ['행정구']\n",
    "le_dict = {}\n",
    "for col in label_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le\n",
    "\n",
    "feature_cols = [\n",
    "    '행정구', '월', '일', '시', '요일', '총생활인구수', '강수', '기온', '습도', '풍속',\n",
    "    '주말구분', '공휴일',\n",
    "    '대여량_1년전', '총생활인구수_1년전',\n",
    "    '대여량_증감', '대여량_증감률', '인구_증감', '인구_증감률'\n",
    "]\n",
    "X = df[feature_cols]\n",
    "y = df['대여량_class']\n",
    "\n",
    "num_cols = [\n",
    "    '총생활인구수', '강수', '기온', '습도', '풍속',\n",
    "    '대여량_1년전', '총생활인구수_1년전',\n",
    "    '대여량_증감', '대여량_증감률', '인구_증감', '인구_증감률'\n",
    "]\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "train_idx = df['연'] <= 2022\n",
    "val_idx = (df['연'] == 2023)\n",
    "test_idx = (df['연'] == 2024)\n",
    "X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
    "\n",
    "# ----------------- DNN 모델 -----------------\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "num_classes = y.nunique()\n",
    "input_dim = X_train.shape[1]\n",
    "model_dnn = Sequential([\n",
    "    Dense(256, input_dim=input_dim, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model_dnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('best_dnn_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "history = model_dnn.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=512,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[es, mc],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 학습과정 그래프를 subplots와 twinx로 시각화\n",
    "fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "ax1.plot(history.history['loss'], label='Train Loss', color='tab:blue')\n",
    "ax1.plot(history.history['val_loss'], label='Val Loss', color='tab:cyan')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(history.history['accuracy'], label='Train Acc', color='tab:orange')\n",
    "ax2.plot(history.history['val_accuracy'], label='Val Acc', color='tab:red')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.title('DNN Training Loss & Accuracy (twin y-axis)')\n",
    "plt.show()\n",
    "\n",
    "# --------- 평가(검증/테스트) 및 시각화 ---------\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred_val = np.argmax(model_dnn.predict(X_val), axis=1)\n",
    "val_acc = (y_pred_val == y_val.values).mean()\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(\"Validation Classification Report\")\n",
    "print(classification_report(y_val, y_pred_val, digits=3))\n",
    "plt.figure(figsize=(8,6))\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix (Validation Set)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "y_pred_test = np.argmax(model_dnn.predict(X_test), axis=1)\n",
    "test_acc = (y_pred_test == y_test.values).mean()\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"Test Classification Report\")\n",
    "print(classification_report(y_test, y_pred_test, digits=3))\n",
    "plt.figure(figsize=(8,6))\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('Confusion Matrix (Test Set)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# 모델, 스케일러, 인코더, feature 리스트 저장\n",
    "model_dnn.save('best_dnn_model.h5')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(le_dict, 'labelencoders.pkl')\n",
    "joblib.dump(feature_cols, 'featurecols.pkl')\n",
    "joblib.dump(num_cols, 'numcols.pkl')\n",
    "\n",
    "# 클래스 index → 대여량 구간 변환 함수\n",
    "def class_to_range(pred_class):\n",
    "    global bins\n",
    "    pred_class = int(pred_class)\n",
    "    left = int(bins[pred_class])\n",
    "    right = int(bins[pred_class+1]) if pred_class+1 < len(bins) else '+'\n",
    "    return f\"{left} ~ {right}\"\n",
    "\n",
    "# 예측 함수 (DNN 버전)\n",
    "def predict_bike_demand(행정구, 강수, 습도, 풍속, 기온):\n",
    "    \"\"\"\n",
    "    행정구명, 강수, 습도, 풍속, 기온을 입력받아\n",
    "    해당 시간의 공공자전거 대여량 예측 클래스를 반환하는 함수\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    from datetime import datetime, timedelta\n",
    "    import holidays\n",
    "    import tensorflow as tf\n",
    "\n",
    "    le_dict = joblib.load('labelencoders.pkl')\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "    feature_cols = joblib.load('featurecols.pkl')\n",
    "    num_cols = joblib.load('numcols.pkl')\n",
    "    model = tf.keras.models.load_model('best_dnn_model.h5')\n",
    "\n",
    "    now = datetime.now()\n",
    "    년, 월, 일, 시 = now.year, now.month, now.day, now.hour\n",
    "    요일 = now.weekday()\n",
    "    주말구분 = 1 if 요일 >= 5 else 0\n",
    "    kr_holidays = holidays.KR(years=[년])\n",
    "    공휴일 = int(now.date() in kr_holidays)\n",
    "\n",
    "    dt_1y = now - timedelta(days=365)\n",
    "    년_1, 월_1, 일_1, 시_1 = dt_1y.year, dt_1y.month, dt_1y.day, dt_1y.hour\n",
    "    row_1y = df[(df['행정구'] == le_dict['행정구'].transform([행정구])[0]) &\n",
    "                (df['연'] == 년_1) & (df['월'] == 월_1) &\n",
    "                (df['일'] == 일_1) & (df['시'] == 시_1)]\n",
    "    if row_1y.empty:\n",
    "        raise ValueError('1년 전 데이터가 존재하지 않습니다.')\n",
    "    대여량_1년전 = row_1y['대여량'].values[0]\n",
    "    총생활인구수_1년전 = row_1y['총생활인구수'].values[0]\n",
    "    총생활인구수 = 총생활인구수_1년전\n",
    "\n",
    "    대여량_증감 = 0\n",
    "    대여량_증감률 = 0\n",
    "    인구_증감 = 0\n",
    "    인구_증감률 = 0\n",
    "\n",
    "    input_dict = {\n",
    "        '행정구': le_dict['행정구'].transform([행정구])[0],\n",
    "        '월': 월,\n",
    "        '일': 일,\n",
    "        '시': 시,\n",
    "        '요일': 요일,\n",
    "        '총생활인구수': 총생활인구수,\n",
    "        '강수': 강수,\n",
    "        '기온': 기온,\n",
    "        '습도': 습도,\n",
    "        '풍속': 풍속,\n",
    "        '주말구분': 주말구분,\n",
    "        '공휴일': 공휴일,\n",
    "        '대여량_1년전': 대여량_1년전,\n",
    "        '총생활인구수_1년전': 총생활인구수_1년전,\n",
    "        '대여량_증감': 대여량_증감,\n",
    "        '대여량_증감률': 대여량_증감률,\n",
    "        '인구_증감': 인구_증감,\n",
    "        '인구_증감률': 인구_증감률\n",
    "    }\n",
    "    input_df = pd.DataFrame([input_dict])\n",
    "    input_df[num_cols] = scaler.transform(input_df[num_cols])\n",
    "    X_input = input_df[feature_cols].values\n",
    "    pred = model.predict(X_input)\n",
    "    pred_class = int(np.argmax(pred, axis=1)[0])\n",
    "    return pred_class\n",
    "\n",
    "# 사용 예시\n",
    "pred = predict_bike_demand('강남구', 0.2, 60, 2.5, 21)\n",
    "print(pred, class_to_range(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4223d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
