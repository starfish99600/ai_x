{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e07e7f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:85% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:85% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64943de",
   "metadata": {},
   "source": [
    "**<font size=\"6\" color=\"red\">ch1. 허깅페이스</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51e7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12279587",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">ch.01 허깅페이스</span>\n",
    "- Transformers라이브러리 내 pipeline()함수\n",
    "- INference API : key를 사용\n",
    "# 1. 텍스트 기반 감정분석(긍정/부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f087b378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"text-classification\",\n",
    "                     model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "980be8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8857762217521667}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"이 영화는 정말 최고였어요. 감독적이고 연기가 대단해요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba8cb9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POSITIVE', 'NEGATIVE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classifier([\"I've been waiting for a HuggingFace course my whole life.\",\n",
    "           \"I hate this so much!\"])\n",
    "[r.get('label')  for r in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "656f1042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"sentiment-analysis\",\n",
    "                     model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "classifier([\"I've been waiting for a HuggingFace course my whole life.\",\n",
    "           \"I hate this so much!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6885ae14",
   "metadata": {},
   "source": [
    "# 2. 제로-샷 분류(zero-shot-classification)\n",
    "- 비지도학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b046f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       " 'labels': ['urgent', 'phone', 'computer', 'tablet'],\n",
       " 'scores': [0.5049759745597839,\n",
       "  0.4800756573677063,\n",
       "  0.012633566744625568,\n",
       "  0.0023149014450609684]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                     \"facebook/bart-large-mnli\")\n",
    "classifier(\"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "          candidate_labels=[\"phone\", \"urgent\", \"tablet\", \"computer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7417995e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the transformers library.',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9053573608398438, 0.07259701192378998, 0.02204558067023754]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제시된 문장이 어떤 분류에 속할지 \n",
    "classifier(\n",
    "    \"This is a course about the transformers library.\",\n",
    "    candidate_labels=[\"education\", \"business\", \"politics\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c584c",
   "metadata": {},
   "source": [
    "# 3. text 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df511415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6a67e43e3646399c4a38a16bacb36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257ac32b97714723a4e56eea2a9055d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457816e3db9e4802a49607d917ef3ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89471841b3824f838a360d97386aeed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed251905704b44fd96707e697f3500fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5387609f16436c83e0c03a50330802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course. We will teach you how to easily generate a small set of code using Lua. We will provide examples of a more sophisticated but efficient form of Lua that requires a little more time than the first two.\\n\\n\\nPlease share your Lua'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(task=\"text-generation\",\n",
    "                    model=\"gpt2\") # 허깅페이스에는 gpt2까지\n",
    "generator(\"In this course. We will teach you how to\",\n",
    "         pad_token_id=generator.tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79780c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 50, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '이 과정은 다음과 같은 방법을 알려드려요. �'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"이 과정은 다음과 같은 방법을 알려드려요. \",\n",
    "         pad_token_id=generator.tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900229e6",
   "metadata": {},
   "source": [
    "# 4. 마스크 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf34c2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.06455978751182556,\n",
       "  'token': 1816,\n",
       "  'token_str': ' girl',\n",
       "  'sequence': \"I'm going to pospital and meet a girl\"},\n",
       " {'score': 0.03595840185880661,\n",
       "  'token': 1441,\n",
       "  'token_str': ' friend',\n",
       "  'sequence': \"I'm going to pospital and meet a friend\"},\n",
       " {'score': 0.02911880798637867,\n",
       "  'token': 2173,\n",
       "  'token_str': ' guy',\n",
       "  'sequence': \"I'm going to pospital and meet a guy\"},\n",
       " {'score': 0.025361763313412666,\n",
       "  'token': 6429,\n",
       "  'token_str': ' lady',\n",
       "  'sequence': \"I'm going to pospital and meet a lady\"},\n",
       " {'score': 0.021882314234972,\n",
       "  'token': 36289,\n",
       "  'token_str': ' prostitute',\n",
       "  'sequence': \"I'm going to pospital and meet a prostitute\"}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", \"distilroberta-base\")\n",
    "unmasker(\"I'm going to pospital and meet a <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55daea65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.036119475960731506,\n",
       "  'token': 265,\n",
       "  'token_str': ' business',\n",
       "  'sequence': \"Hello, I'm a business model\"},\n",
       " {'score': 0.02683805488049984,\n",
       "  'token': 18150,\n",
       "  'token_str': ' freelance',\n",
       "  'sequence': \"Hello, I'm a freelance model\"}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Hello, I'm a <mask> model\",\n",
    "        top_k=2) #top_k를 안 주면 5개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd710e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.06705840677022934,\n",
       "  'token': 4827,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': \"hello, i'm a fashion model\"},\n",
       " {'score': 0.058972690254449844,\n",
       "  'token': 2047,\n",
       "  'token_str': 'new',\n",
       "  'sequence': \"hello, i'm a new model\"}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# google-bert/bert-base-uncased사용을 위해 key 발부\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline(task=\"fill-mask\",\n",
    "                   model=\"google-bert/bert-base-uncased\")\n",
    "unmasker(\"Hello, I'm a [MASK] model\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "746ef693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# print(os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba890dbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InferenceClient' from 'huggingface_hub' (C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferenceClient\n\u001b[0;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m InferenceClient(provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf-inference\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m                          api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHF_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mfill_mask(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm a [MASK] model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-bert/bert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'InferenceClient' from 'huggingface_hub' (C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient(provider=\"hf-inference\",\n",
    "                         api_key=os.environ['HF_TOKEN'])\n",
    "result = client.fill_mask(\n",
    "    \"Hello, I'm a [MASK] model\",\n",
    "    model=\"google-bert/bert-base-uncased\",\n",
    "    top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e508fc22",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m [r\u001b[38;5;241m.\u001b[39msequence \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m [\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'sequence'"
     ]
    }
   ],
   "source": [
    "[r.sequence for r in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06cf6f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6111933d2d184a74b1ff68ec92ae8faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 다국어지원 모델도 한글 지원 만족스럽지 않을 수 있음\n",
    "unmasker = pipeline(\"fill-mask\",\n",
    "                   model=\"bert-base-multilingual-cased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ec386",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker(\"안녕하세요? 나는 [MASK] 모델입니다\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21af75d",
   "metadata": {},
   "source": [
    "# 5. 개체명 인식(NER : Naned Entity REcognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dee6300d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6599fa58bde54a6489e5c2f458507d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner = pipeline(task=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf225b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99940455,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9983193,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9966254,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9993112,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9817053,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.96778005,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9814528,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.94097567,\n",
       "  'index': 16,\n",
       "  'word': 'Brook',\n",
       "  'start': 49,\n",
       "  'end': 54},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.8209802,\n",
       "  'index': 17,\n",
       "  'word': '##ly',\n",
       "  'start': 54,\n",
       "  'end': 56}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"My name is Sylvain and I work at Hugging Face in Brookly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913777eb",
   "metadata": {},
   "source": [
    "# 6. 질의응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d973429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ea93a9265848d58429d9f012904272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "question_answer = pipeline(\"question-answering\",\n",
    "                          \"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c30f602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5976612567901611, 'start': 29, 'end': 37, 'answer': 'Facebook'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Tom and I work at Facebook in Brooklyn\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c721d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Facebook'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "context=\"My name is Tom and I work at Facebook in Brooklyn\"\n",
    "context[29:37]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16472303",
   "metadata": {},
   "source": [
    "# 7. 문서요약\n",
    "- 현재 torch 2.6이상 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5d62441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "853ff0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c39ecff4c024d9aa46859aa5e2ae36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model facebook/bart-large-cnn with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large-cnn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py:727\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;66;03m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    726\u001b[0m model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 727\u001b[0m framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    728\u001b[0m     model,\n\u001b[0;32m    729\u001b[0m     model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    730\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    731\u001b[0m     framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    732\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    735\u001b[0m )\n\u001b[0;32m    737\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    738\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:266\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m framework, model\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model facebook/bart-large-cnn with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>)."
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(task=\"summarization\",\n",
    "                     model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae48b2",
   "metadata": {},
   "source": [
    "# 8. 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3493236c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 한 -> 영\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ko2en \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHelsinki-NLP/opus-mt-ko-en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 영 -> 한(?)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m en2ko \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m                  model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-tc-big-en-ko\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py:801\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    798\u001b[0m             tokenizer_identifier \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[0;32m    799\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\n\u001b[1;32m--> 801\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    802\u001b[0m             tokenizer_identifier, use_fast\u001b[38;5;241m=\u001b[39muse_fast, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs\n\u001b[0;32m    803\u001b[0m         )\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_feature_extractor:\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# Try to infer feature extractor from model or config name (if provided as str)\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:642\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 642\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    643\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    645\u001b[0m             )\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    650\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "# 한 -> 영\n",
    "ko2en = pipeline(\"translation\",\n",
    "                 model=\"Helsinki-NLP/opus-mt-ko-en\")\n",
    "\n",
    "# 영 -> 한(?)\n",
    "en2ko = pipeline(\"translation\",\n",
    "                 model=\"Helsinki-NLP/opus-mt-tc-big-en-ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aadfdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 문장\n",
    "ko_sentence = \"이 문장을 영어로 번역해 주세요\"\n",
    "en_sentence = \"I enjoy learning about AI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a487618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4decf051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df63c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e7a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d4c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de6eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5064ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97710b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e9b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a338cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ec3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba265abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
