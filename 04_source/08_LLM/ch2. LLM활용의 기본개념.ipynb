{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc06f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:85% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:85% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f94d39",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">ch2. LMM활용의 기본개념</span>\n",
    "\n",
    "# 1. LMM을 활용하여 답변 생성하기\n",
    "## 1) ollama 이용한 로컬 LLM 이용\n",
    "성능은 GPT, Claude 같은 모델보다 떨어지나, 개념 설명을 위해 open source 모델 사용\n",
    "\n",
    "### ⓐ ollama.com 다운로드 -> 설치 -> 모델 pull\n",
    "- ollama pull deepseek-r1:1.5b (window키+R : powershell창)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78526279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nThe capital of Korea is Daejeon, which is also known as Deulcheong. It is located in South Korea and serves as the administrative center for several departments.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-06-26T03:12:55.066023Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4725720600, 'load_duration': 2389362700, 'prompt_eval_count': 11, 'prompt_eval_duration': 269541600, 'eval_count': 41, 'eval_duration': 2064745900, 'model_name': 'deepseek-r1:1.5b'}, id='run--1ca1c03c-9089-489e-91a4-494b636b5666-0', usage_metadata={'input_tokens': 11, 'output_tokens': 41, 'total_tokens': 52})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "result = llm.invoke(\"what is the capital of korea?\")\n",
    "result # 추론모델<think>~<think>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45acc5",
   "metadata": {},
   "source": [
    "### ollama.com 다운로드 -> 설치 -> 모델 pull\n",
    "- ollama run llama3.2:1b (window키+R : powershell창)\n",
    "- llama : 공식적으로 한글지원 안 됨. (llama 3.1 405b한글지원 가능-> llama3.3 70b)\n",
    "- exaone : 공식적으로 한글지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530f4b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Korea is Seoul.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T02:17:40.6186846Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3293018400, 'load_duration': 2239838300, 'prompt_eval_count': 33, 'prompt_eval_duration': 640890500, 'eval_count': 8, 'eval_duration': 410749500, 'model_name': 'llama3.2:1b'}, id='run--433901b1-9ee5-4df0-aee5-ddf78fdef8fc-0', usage_metadata={'input_tokens': 33, 'output_tokens': 8, 'total_tokens': 41})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "result = llm.invoke(\"what is the capital of korea?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40e417f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Korea is Seoul.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80598bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국의 수도는 Seoul입니다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke(\"한국의 수도는 어디야?\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6bd9f",
   "metadata": {},
   "source": [
    "## 2) openai 활용\n",
    "- pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5341c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOllama(model=\"gpt-4o-mini\")\n",
    "# result = llm.invoke(\"what is the capital of korea?\")\n",
    "# result => 에러이유 : OPENAI_API_KEY 환경변수 부재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a841ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경변수 가져오기\n",
    "from dotenv import load_dotenv\n",
    "# import os\n",
    "load_dotenv()\n",
    "# os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97cce8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코랩에서 OPENAI_API_KEY 읽어오기(.env못씀)\n",
    "# 보안키 추가 후\n",
    "# from google.colab import userdata\n",
    "# userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5cad40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='대한민국의 수도는 서울입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 18, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BmBeVbxeIv9EKnOFSfkmVSR3ntHed', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--7ee046b2-6d48-4a2c-8415-9a14c6928c25-0', usage_metadata={'input_tokens': 18, 'output_tokens': 8, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\",\n",
    "                # openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    "                )\n",
    "llm.invoke(\"What is the capital of Korea? Answer me in Korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "067e1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델의 키가 OPENAI_API_KEY는 아님\n",
    "# Claude -> Anthropic\n",
    "# Azure, upstage, Bedrock : 에러 메세지 참조하여 환경변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "766d7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import AzureOpenAI\n",
    "# llm = AzureOpenAI(model=\"gpt-4.1-mini\")\n",
    "# 에러를 내면 OPENAI_API_VERSION 환경변수가 필요하다는 메세지가 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f94347f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "# llm.invoke(\"What is the capital of Korea?\")\n",
    "# 에러 메세지를 봐도 환경변수 이름을 알 수 X -> ChatAnthropic 검색 후\n",
    "# langchain docs에서 명시한 ANTHROPIC_API_KEY 이름의 환경변수 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3d9f2",
   "metadata": {},
   "source": [
    "# 2. 렝체인 스타일로 프롬프트 작성하기\n",
    "- 프롬프트 : lim호출시 쓰는 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e913529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "# llm.invoke(0)\n",
    "# 프로프트 타입 : 스트링, PromptValue, BaseMessage 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b215bf",
   "metadata": {},
   "source": [
    "## 1) 기본 프롬프트 템플릿 사용\n",
    "- PromptTemplate을 사용하여 변수가 포함된 템플릿 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e17cdf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of Korea'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The capital of South Korea is Seoul. However, it's worth noting that North Korea also claims Seoul as its capital. This is due to a joint statement made by the two countries in 2018, which recognized Seoul as their respective capitals.\\n\\nThat being said, Pyongyang in North Korea is actually considered the capital of the Democratic People's Republic of Korea (North Korea).\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T05:32:08.5349185Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4596323000, 'load_duration': 21122700, 'prompt_eval_count': 31, 'prompt_eval_duration': 67794600, 'eval_count': 75, 'eval_duration': 4507334600, 'model_name': 'llama3.2:1b'}, id='run--4dc9092b-d8a6-4970-8158-90b7777ea06b-0', usage_metadata={'input_tokens': 31, 'output_tokens': 75, 'total_tokens': 106})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}\", #{}안의 값을 새로운 값으로 대입 가능\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "prompt = prompt_template.invoke({\"country\":\"Korea\"})\n",
    "print(prompt)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195d5bd5",
   "metadata": {},
   "source": [
    "## 2) 메세지 기반 프롬프트 작성\n",
    "- BaseMessage리스트\n",
    "- BaseMessage 상속받은 클래스 : AIMessage, HummanMessage, SystemMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ae34e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I think there might be some confusion. The capital of France is Paris, not Italy or Korea. Paris has been the capital of France since 987 AD, when Charles III the Simple seized the city from the Capetian dynasty and declared it the capital of his kingdom.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-25T05:52:08.8657607Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3320191300, 'load_duration': 20456100, 'prompt_eval_count': 86, 'prompt_eval_duration': 61608600, 'eval_count': 56, 'eval_duration': 3236546100, 'model_name': 'llama3.2:1b'}, id='run--57507419-4368-4687-85c1-16b39150c2a4-0', usage_metadata={'input_tokens': 86, 'output_tokens': 56, 'total_tokens': 142})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "message_list =[\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"), \n",
    "    HumanMessage(content=\"What is the capital of Italy?\"),\n",
    "    AIMessage(content=\"The capital of Italy is Rome.\"),\n",
    "    HumanMessage(content=\"What is the capital of Korea?\"),\n",
    "    AIMessage(content=\"The capital of Italy is Seoul.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\")\n",
    "]\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "953c6c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Italy?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Rome.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Korea?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Seoul.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of {country}?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "message_list =[\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"), \n",
    "    HumanMessage(content=\"What is the capital of Italy?\"),\n",
    "    AIMessage(content=\"The capital of Italy is Rome.\"),\n",
    "    HumanMessage(content=\"What is the capital of Korea?\"),\n",
    "    AIMessage(content=\"The capital of Italy is Seoul.\"),\n",
    "    HumanMessage(content=\"What is the capital of {country}?\")\n",
    "]\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages(message_list)\n",
    "prompt = chatPromptTemplate.invoke({'country':'Korea'})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c2511",
   "metadata": {},
   "source": [
    "## 3) ChatPromptTemplate 사용\n",
    "- BaseMessage 리스트 -> 튜플리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce679329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느나라 수도가 궁금하세요?한국\n",
      "프롬프트: messages=[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Italy?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Rome.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Korea?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Seoul.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of {country}?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'한국의 수도는 Seoul입니다.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 BaseMessage를 수정\n",
    "ChatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"넌 훌룡한 도우미야!\"), \n",
    "    (\"human\", \"{country}의 수도가 어디야?\"),\n",
    "])\n",
    "country = input(\"어느나라 수도가 궁금하세요?\")\n",
    "prompt = chatPromptTemplate.invoke({\"country\":\"France\"})\n",
    "print(\"프롬프트:\" ,prompt)\n",
    "reault = llm.invoke(prompt)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9244e",
   "metadata": {},
   "source": [
    "# 3. 답변의 형식을 컨트롤하기\n",
    "- invoke 실행결과는 AIMessage() -> String이나 json, 객체 : outputParser이용\n",
    "\n",
    "## 1) 문자열 출력 파서 이용\n",
    "- StrOutputParser를 사용하여 LLM출력(AIMessage)을 단순 문자열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d9fe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프롬프트 : text='What is the capital of Korea. Return the name of the city only'\n",
      "llm 결과 : <class 'langchain_core.messages.ai.AIMessage'> content='Seoul' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-26T01:52:05.9866509Z', 'done': True, 'done_reason': 'stop', 'total_duration': 549478300, 'load_duration': 27956600, 'prompt_eval_count': 39, 'prompt_eval_duration': 401854600, 'eval_count': 3, 'eval_duration': 119667100, 'model_name': 'llama3.2:1b'} id='run--8b2728a0-fac7-4661-8da7-238eb378d1b5-0' usage_metadata={'input_tokens': 39, 'output_tokens': 3, 'total_tokens': 42}\n",
      "파서 결과 : Seoul\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Return the name of the city only\",\n",
    "    input_varibles = [\"country\"]\n",
    ")\n",
    "# 프롬프트 템플릿에 값 주입\n",
    "prompt = prompt_template.invoke({\"country\":\"Korea\"})\n",
    "print('프롬프트 :', prompt)\n",
    "result = llm.invoke(prompt)\n",
    "print('llm 결과 :', type(result), result)\n",
    "# 문자열 출력 파서를 이용하여 llm응답을 단순 문자열 변환\n",
    "output_parsers = StrOutputParser()\n",
    "print('파서 결과 :', output_parsers.invoke(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5f561fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parsers.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b30d541",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOllama' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# PromptTemplate(변수설정) => ChatPromptTemplate(변수설정, system과 모범답안 지정)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOllama\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2:1b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m chat_prompt_template \u001b[38;5;241m=\u001b[39m ChatPromptTemplate([\n\u001b[0;32m      5\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant with expertise in South Korea.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of \u001b[39m\u001b[38;5;132;01m{country}\u001b[39;00m\u001b[38;5;124m? Return the name if the city only.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m ])\n\u001b[0;32m      9\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m StrOutputParser()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatOllama' is not defined"
     ]
    }
   ],
   "source": [
    "# PromptTemplate(변수설정) => ChatPromptTemplate(변수설정, system과 모범답안 지정)\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant with expertise in South Korea.\"),\n",
    "    (\"human\", \"What is the capital of {country}? Return the name if the city only.\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "output_parser.invoke(llm.invoke(chat_prompt_template.invoke({\"country\":\"korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10947815",
   "metadata": {},
   "source": [
    "## 2) Json 출력 파서 이용\n",
    "- json()으로 응답하기를 원하지만, 우선 어떤 형식으로 반환되는지 확인\n",
    "- {\"name\":\"홍\",\"age\":22}(json) /{'name':'홍','age':22}(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad0c115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'> text='Give following information about Korea\\n    - Capital\\n    - Population\\n    - Language\\n    - Currency\\n    return it is Json format and return the JSON dictionary only'\n",
      "<class 'langchain_core.prompt_values.StringPromptValue'> content='```json\\n{\\n    \"capital\": \"Seoul\",\\n    \"population\": 51.8,\\n    \"language\": \"Korean\",\\n    \"currency\": \"KRW\"\\n}\\n```' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-26T01:51:58.3277123Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3234803600, 'load_duration': 28523800, 'prompt_eval_count': 59, 'prompt_eval_duration': 738891100, 'eval_count': 40, 'eval_duration': 2466824000, 'model_name': 'llama3.2:1b'} id='run--f2733cc6-ac5c-4bb4-925e-a1035c8796e4-0' usage_metadata={'input_tokens': 59, 'output_tokens': 40, 'total_tokens': 99}\n",
      "{'capital': 'Seoul', 'population': 51.8, 'language': 'Korean', 'currency': 'KRW'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "count_detail_prompt = PromptTemplate(\n",
    "    template= \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    return it is Json format and return the JSON dictionary only\"\"\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "prompt = count_detail_prompt.invoke({\"country\":\"Korea\"})\n",
    "print(type(prompt), prompt)\n",
    "# Json output 파서\n",
    "output_parsers = JsonOutputParser()\n",
    "ai_message = llm.invoke(prompt)\n",
    "print(type(prompt), ai_message)\n",
    "json_result = output_parsers.invoke(ai_message)\n",
    "print(json_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67627ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': 'Seoul',\n",
       " 'population': 51.8,\n",
       " 'language': 'Korean',\n",
       " 'currency': 'Korean Won'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_detail_prompt = PromptTemplate(\n",
    "    template= \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    return it is Json format and return the JSON dictionary only\"\"\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "output_parsers = JsonOutputParser()\n",
    "info = output_parsers.invoke(llm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"})))\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33afc359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d02ebf",
   "metadata": {},
   "source": [
    "## 3) 구조화된 출력 사용\n",
    "- Pydantic 모델을 사용하여 LLM 출력을 구조화된 형식으로 받기(JsonParser보다 훨씬 안정적)\n",
    "- Pydantic : 데이터 유효성검사, 설정관리를 간편하게 해주는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f40c6c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.User object at 0x000002485A319DB0>\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "class User:\n",
    "    def __init__(self, id, name, is_active=True):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.is_active = is_active\n",
    "user = User(1, \"홍길동\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "72568e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1 name='홍길동' is_activate=True\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class User(BaseModel):\n",
    "    # gt=0:id>0, ge=0:is>=0, lt=0:id<0, le=0:le<=0\n",
    "    id:int   = Field(gt=0,                description=\"id\")\n",
    "    name:str = Field(min_length=2,        description=\"name\")\n",
    "    is_activate:bool = Field(default=True,description=\"id활성화\")\n",
    "user = User(id=\"1\", name=\"홍길동\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "343e4a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CountryDetail"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_detail_prompt = PromptTemplate(\n",
    "    template= \"\"\"Give following information about {country}\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    return it is Json format and return the JSON dictionary only\"\"\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "class CountryDetail(BaseModel):\n",
    "    capital:str     = Field(description=\"the capital of the country\")\n",
    "    population:int  = Field(description=\"the population of the country\")\n",
    "    language:str    = Field(description=\"the language of the country\")\n",
    "    currency:str    = Field(description=\"the currency of the country\")\n",
    "# 출력 형식 파서 + LLM\n",
    "structedllm = llm.with_structured_output(CountryDetail)\n",
    "\n",
    "# output_parsers = JsonOutputParser()\n",
    "# output_parsers.invoke(llm.invoke(country_detail_prompt({\"country\":\"Korea\"})))\n",
    "\n",
    "info = structedllm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"}))\n",
    "type(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06246a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital='Seoul' population=51 language='Korean' currency='South Korean won'\n",
      "Seoul 51\n"
     ]
    }
   ],
   "source": [
    "print(info)\n",
    "print(info.capital, info.population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3d63caa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info를 json : {\"capital\":\"Seoul\",\"population\":51,\"language\":\"Korean\",\"currency\":\"South Korean won\"}\n",
      "info를 dict : {'capital': 'Seoul', 'population': 51, 'language': 'Korean', 'currency': 'South Korean won'}\n"
     ]
    }
   ],
   "source": [
    "print('info를 json :', info.model_dump_json()) # json()\n",
    "print('info를 dict :', info.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c07b3b",
   "metadata": {},
   "source": [
    "# 4. LCEL을 활용한 렝체인 생성하기\n",
    "## 1) 문자열 출력 파서를 사용 \n",
    "- invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b388373d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "llm = ChatOllama(model=\"llama3.2:1b\", \n",
    "                 temperature=0) # 일관된 답변\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Return the name of the city only\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842eab74",
   "metadata": {},
   "source": [
    "### 2) LCEL을 사용한 간단한 체인 구성\n",
    "- 파이프연산자(|) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b933ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트템플릿 -> llm -> 출력파서를 연결하는 체인 생성\n",
    "capital_chain = prompt_template | llm | output_parsers\n",
    "# 생성된 체인 invoke\n",
    "capital_chain.invoke({\"country\":\"korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a1bc6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(capital_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4380a",
   "metadata": {},
   "source": [
    "## 3) 복합체인 구성\n",
    "- 여러단계의 추론이 필요한 경우 (체인 연결)\n",
    "- Q. 나라명 (-> 제일 유명한 음식 ->) 음식의 레시피(시험문제로 나올예정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd3f6920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'France.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라설명을 입력하면 나라명을 출력\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "country_prompt = PromptTemplate(\n",
    "    template = \"\"\"Guess the name of the country based on the following information:\n",
    "    {information} \n",
    "    Return the name of the country only\"\"\",\n",
    "    input_variables=[\"\"]\n",
    ")\n",
    "output_parser.invoke(llm.invoke(country_prompt.invoke({\"information\":\n",
    "                                                      \"This country is very famous for its wine\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ddadbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라명 추측 체인 생성\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "# type(country_chain)\n",
    "country_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ca3ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라설명 -> 그 나라 수도 출력(나라명을 찾아서 수도를 출력하는 프로세스를 거침)\n",
    "final_chain = country_chain | capital_chain\n",
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cd5516b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = {\"country\": country_chain} | capital_chain\n",
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ced94bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "final_chain = {\"information\":RunnablePassthrough()} | {\"country\":country_chain} | capital_chain\n",
    "final_chain.invoke(\"This country is very famous for its wine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89992d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Portugal.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트 템플릿에 변수 2\n",
    "# 나라설명을 입력하면 나라명을 출력\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "country_prompt = PromptTemplate(\n",
    "    template = \"\"\"Guess the name of the country in toe {continent} based on the following information:\n",
    "    {information} \n",
    "    Return the name of the country only\"\"\",\n",
    "    input_variables=[\"information\",\"continent\"]\n",
    ")\n",
    "# output_parser.invoke(llm.invoke(country_prompt.invoke({\"information\":\n",
    "#                                                      \"This country is very famous for its wine\",\n",
    "#                                                                    \"continent\":\"Europe\"})))\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "country_chain.invoke({\"information\":\"This country is very famous for its wine\",\n",
    "                     \"continent\":\"Europe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c7787d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lisbon'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = {\"country\" : country_chain} | capital_chain\n",
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\",\n",
    "                     \"continent\":\"Europe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5d3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae9d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9557a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf8649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca07d07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb26ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4fee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb4a484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd615bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad356b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9392fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
